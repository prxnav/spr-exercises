{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Pattern Recognition - Exercise 6: Gaussian processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMGP7sA8B7ak"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define plot function \n",
    "\n",
    "We test it on [this example from sklearn](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-noisy-targets-py)\n",
    "\n",
    "To plot the 95% confidence interval we look up the z-score for the standard normal distribution.\n",
    "Assume a standard normal with mean 0, std 1, then for input value x=1.96 the cumulative density is 0.975.\n",
    "This means 97.5% of all probability mass is to the left of x.\n",
    "If we now cut off left of -1.96 and right of 1.96 we will have 95% of the probability mass inside those boundaries.\n",
    "\n",
    "```python\n",
    "# import scipy.stats\n",
    "# z_score = stats.norm.ppf(0.975)  # ~1.96\n",
    "```\n",
    "\n",
    "Ppf here means percent-point function (inverse of the cumulative density function or CDF) for a standard normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp(mu, cov, x, x_train=None, y_train=None, label=\"Pred\"):\n",
    "    \"\"\"\n",
    "        mu: mean prediction, shape (N, 1)\n",
    "        cov: prediction covariance, shape (N, N)\n",
    "        x: inputs to predict for, shape (N, 1)\n",
    "        x_train: observation inputs, shape (N_obs, 1)\n",
    "        y_train: obersvation labels, shape (N_obs,)\n",
    "    \"\"\"\n",
    "    x, mu = x.ravel(), mu.ravel()  # flatten both to shape (N, )\n",
    "    uncertainty = 1.96 * np.sqrt(np.diag(cov))\n",
    "\n",
    "    plt.fill_between(x, mu + uncertainty, mu - uncertainty, alpha=0.1)\n",
    "    plt.plot(x, mu, label=label)\n",
    "\n",
    "    if x_train is not None:\n",
    "        n_samples = x_train.shape[0]\n",
    "        plt.plot(x_train, y_train, \"rx\", label=f\"{n_samples} train samples\")\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the gp example from sklearn\n",
    "e_x = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n",
    "e_y = np.squeeze(e_x * np.sin(e_x))\n",
    "rng = np.random.RandomState(1)\n",
    "training_indices = rng.choice(np.arange(e_y.size), size=6, replace=False)\n",
    "e_x_train, e_y_train = e_x[training_indices], e_y[training_indices]\n",
    "kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel, optimizer=None)\n",
    "gaussian_process.fit(e_x_train, e_y_train)\n",
    "print(f\"{gaussian_process.kernel_=}\")\n",
    "\n",
    "# run our plot code\n",
    "mu, cov = gaussian_process.predict(e_x, return_cov=True)\n",
    "plot_gp(mu, cov, e_x, e_x_train, e_y_train)\n",
    "plt.show()\n",
    "\n",
    "# compare with sklearn's plot code\n",
    "mean_prediction, std_prediction = gaussian_process.predict(e_x, return_std=True)\n",
    "plt.plot(e_x, e_y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(e_x_train, e_y_train, label=\"Observations\")\n",
    "plt.plot(e_x, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    e_x.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "plt.title(\"Gaussian process regression on noise-free dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfV-0nQUB-rE"
   },
   "source": [
    "## $\\star$ Part 1: Gaussian process\n",
    "\n",
    "Load the points from regression.npz. \n",
    "\n",
    "Estimate the mean prediction and the variance using a Gaussian process and plot both in the style of last weekâ€™s assignment. \n",
    "\n",
    "Use an RBF kernel (a Gaussian function). Play\n",
    "with the hyperparameters (which hyperparameters do you have?) and see\n",
    "the effect in the predictive distribution. \n",
    "\n",
    "Reduce the number of samples and\n",
    "repeat the experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Thb2wsjCUgQ"
   },
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Load data and reshape it into X_train shape (N, 1) and Y_train shape (N,)\n",
    "# Create a suitable range X_test for testing\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Create a function that fits a GP using sklearn and run the function on the data\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Randomly subsample your data and run the GP on [1, 2, 3, 5, 10, 15, 20] datapoints.\n",
    "# What do you observe?\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Go back to the full dataset and play with the other hyperparameters.\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Now, implement your own rbf_kernel function and MyGPRegressor class.\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Run your implementation on the data.\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Play with the beta parameter of MyGPRegressor and also length and sigma_f parameters of rbf_kernel\n",
    "# Observe how these *hyperparameters* change the data fit. Try to reason why you obtain these different results.\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXhy-sKuCVBv"
   },
   "source": [
    "## $\\star \\star \\star$ Part 2: Hyperparameter optimization\n",
    "\n",
    "Can you optimize the hyperparameters? \n",
    "\n",
    "You must calculate the gradient of the log-likelihood with respect to each hyperparameter (this requires some knowledge on matrix calculus). \n",
    "\n",
    "Once you have the gradient you can run a gradient ascent on the hyperparameters. \n",
    "\n",
    "Make sure that the step size in the gradient ascent is not too big, otherwise it will not converge.\n",
    "\n",
    "**Hint:** You can also use [JAX](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html) to compute gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n",
    "\n",
    "\n",
    "length = 1.\n",
    "sigma_f = 1.\n",
    "beta = 100.\n",
    "gp = MyGPRegressorRBF(beta=beta, length=length, sigma_f=sigma_f)\n",
    "gp.fit(x_train, y_train)\n",
    "mu_test, sigma_test = gp.predict(x_test)\n",
    "\n",
    "plot_gp(\n",
    "    mu_test,\n",
    "    sigma_test,\n",
    "    x_test,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def log_likelihood_params(params):\n",
    "    length_, sigma_f_ = params\n",
    "    gp = MyGPRegressorRBF(beta=beta, length=length_, sigma_f=sigma_f_)\n",
    "    gp.fit(x_train, y_train)\n",
    "    ll = gp.log_likelihood()\n",
    "    return ll\n",
    "\n",
    "# gradients = grad(log_likelihood_params)(jnp.array([length, sigma_f]))\n",
    "# print(gradients)\n",
    "\n",
    "\n",
    "# # with warnings.filterwarnings(\"ignore\"):\n",
    "n_steps, report_every = 11, 1\n",
    "alpha = -0.01\n",
    "theta_min = 1e-4\n",
    "for i in range(n_steps):\n",
    "    length_grad, sigma_f_grad = grad(log_likelihood_params)(jnp.array([length, sigma_f]))\n",
    "    new_length = length + alpha * length_grad\n",
    "    new_sigma_f = sigma_f + alpha * sigma_f_grad\n",
    "\n",
    "    # constrain hyperparameters to be positive\n",
    "    new_length = jnp.maximum(new_length, theta_min)\n",
    "    new_sigma_f = jnp.maximum(new_sigma_f, theta_min)\n",
    "\n",
    "    if i % report_every == 0:\n",
    "        ll = log_likelihood_params(jnp.array([length, sigma_f]))\n",
    "        print(f\"{i=} {ll=} {length_grad=} {sigma_f_grad=} {length=} {sigma_f=}\")\n",
    "        # print(gp.kernel_)\n",
    "        gp = MyGPRegressorRBF(beta=beta, length=length, sigma_f=sigma_f)\n",
    "        gp.fit(x_train, y_train)\n",
    "        mu, cov = gp.predict(x_test)\n",
    "        plot_gp(mu, cov, x_test, x_train=x_train, y_train=y_train)\n",
    "        plt.title(f\"{i=} likelihood: {ll:.3e}\")\n",
    "        plt.show()\n",
    "\n",
    "    length = new_length\n",
    "    sigma_f = new_sigma_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def calc_loglikelihood(X_train, Y_train, length=1, sigma_f=1, alpha = 0.05):\n",
    "    \"\"\"\n",
    "    Returns log-marginal likelihood of theta for training data as well as the gradient\n",
    "    \"\"\"\n",
    "    theta = [length, sigma_f]  # array containing the hyperparameters\n",
    "    kernel = ConstantKernel(sigma_f, (1e-2, 1e2)) * RBF(length, (1e-2, 1e2))\n",
    "    # kernel = get_rbf_kernel(length=length, sigma_f=sigma_f)\n",
    "\n",
    "    # rbf = C(sigma_f) * RBF(length_scale=length)\n",
    "    # rbf = RBF(length_scale=sigma)\n",
    "    # rbf = partial(rbf_kernel, l=length, sigma_f=sigma_f)\n",
    "    # rbf = get_rbf_kernel(length=length, sigma_f=sigma_f)\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, optimizer=None, alpha=alpha)\n",
    "    # x = X[:, 0][:, np.newaxis]\n",
    "    gp.fit(X_train, Y_train)\n",
    "\n",
    "    ll, ll_grad = gp.log_marginal_likelihood(\n",
    "        theta=theta, eval_gradient=True, clone_kernel=True\n",
    "    )\n",
    "    # ll, ll_grad = gp.log_marginal_likelihood(theta=theta, eval_gradient=True, clone_kernel=False)\n",
    "\n",
    "    return gp, ll, ll_grad\n",
    "\n",
    "\n",
    "# gradient asscent\n",
    "# I'm having problems here , depending on the initial values chosen, the gradient asscent always seems to send one of the hyperparameters to 0 and then negative which causes an error\n",
    "# I've tried switching up the learning rate but all that does is delay the error\n",
    "\n",
    "# theta = [0.5, 1]\n",
    "theta = np.array([1, 1])\n",
    "theta_min = np.ones_like(theta) * 1e-3\n",
    "\n",
    "\n",
    "# with warnings.filterwarnings(\"ignore\"):\n",
    "n_steps, report_every = 1001, 100\n",
    "if True:\n",
    "    for i in range(n_steps):\n",
    "        alpha = 0.01\n",
    "        # alpha = 0.01/i   # reduces the learning rate over time\n",
    "        gp, ll, ll_grad = calc_loglikelihood(\n",
    "            X_train, Y_train, length=theta[0], sigma_f=theta[1]\n",
    "        )\n",
    "        new_theta = theta + alpha * ll_grad\n",
    "        new_theta = np.maximum(new_theta, theta_min)  # constrain hyperparameters to be positive\n",
    "        if np.any(np.isnan(new_theta)):\n",
    "            print(f\"NaN detected, aborting. {ll=} {ll_grad=} {new_theta=}\")\n",
    "            break\n",
    "\n",
    "        if i % report_every == 0:\n",
    "            print(i, \"log likelihood =\", ll, \"grad =\", ll_grad, \"hyperparams =\", theta)\n",
    "            print(gp.kernel_)\n",
    "            mu, cov = gp.predict(X_test, return_cov=True)\n",
    "            plot_gp(mu, cov, X_test, X_train=X_train, Y_train=Y_train)\n",
    "            plt.title(f\"{i=} likelihood: {ll:.3e}\")\n",
    "            plt.show()\n",
    "        theta = new_theta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ex6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
